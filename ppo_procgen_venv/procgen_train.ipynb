{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-23T05:01:30.182528Z",
     "start_time": "2025-10-23T05:01:29.532626Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from procgen import ProcgenEnv\n",
    "\n",
    "from utils.envs import make_ProcgenEnvs\n",
    "\n",
    "from agent.discrete_ppo import PPO\n",
    "from agent.models import ImpalaModel, CNNBase"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:01:30.242353Z",
     "start_time": "2025-10-23T05:01:30.224852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "e63b9c685613f286",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:01:31.071570Z",
     "start_time": "2025-10-23T05:01:30.288541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('making envs...')\n",
    "start_level = 0\n",
    "num_levels = 200\n",
    "num_test_levels = 200\n",
    "num_envs=32\n",
    "\n",
    "train_envs = make_ProcgenEnvs(num_envs=num_envs,\n",
    "                              env_name='coinrun',\n",
    "                              start_level=start_level,\n",
    "                              num_levels=num_levels,\n",
    "                              distribution_mode='easy',\n",
    "                              use_generated_assets=False,\n",
    "                              use_backgrounds=True,\n",
    "                              restrict_themes=False,\n",
    "                              use_monochrome_assets=False,\n",
    "                              rand_seed=0,\n",
    "                              device=device)\n",
    "\n",
    "test_envs = make_ProcgenEnvs(num_envs=1,\n",
    "                             env_name='coinrun',\n",
    "                             start_level=start_level + num_levels,\n",
    "                             num_levels=num_test_levels,\n",
    "                             distribution_mode='easy',\n",
    "                             use_generated_assets=False,\n",
    "                             use_backgrounds=True,\n",
    "                             restrict_themes=False,\n",
    "                             use_monochrome_assets=False,\n",
    "                             rand_seed=0,\n",
    "                             device=device)"
   ],
   "id": "7741c4d087eacab9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making envs...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:01:31.075511Z",
     "start_time": "2025-10-23T05:01:31.074195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# obs = train_envs.reset()\n",
    "# print(train_envs.observation_space.shape)"
   ],
   "id": "ed66ea1ea3dd6017",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:01:31.129192Z",
     "start_time": "2025-10-23T05:01:31.121927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_agent(env, agent, num_levels=200):\n",
    "    sum_reward = 0\n",
    "    max_ep_len = 1000\n",
    "    for _ in range(num_levels):\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            action, _, _ = agent.select_action(state)\n",
    "            state, reward, done, _ = env.step(action.detach().cpu().numpy())\n",
    "\n",
    "            sum_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return sum_reward / num_levels"
   ],
   "id": "c9f6e3e81acea721",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:01:31.659220Z",
     "start_time": "2025-10-23T05:01:31.175631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ppo_agent = PPO(state_dim=(num_envs, 3, 64, 64), action_dim=15, actor_critic_model=ImpalaModel, lr=5e-4, gamma=0.99, K_epochs=3,\n",
    "                eps_clip=0.2, use_gae=True, gae_lambda=0.95, mini_batch_size=512 * num_envs // 8, use_clipped_value_loss=True, device=device)"
   ],
   "id": "4897d702796d3093",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T05:18:07.090061Z",
     "start_time": "2025-10-23T05:01:31.663956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_steps = 25_000_000\n",
    "update_timestep = 512\n",
    "summary_freq = 10_000\n",
    "\n",
    "time_step = 0\n",
    "running_reward = 0\n",
    "running_episodes = num_envs\n",
    "\n",
    "state = train_envs.reset()\n",
    "while time_step * num_envs < num_steps:\n",
    "\n",
    "    # select action with policy\n",
    "    with torch.no_grad():\n",
    "        action, action_logprob, state_val = ppo_agent.select_action(state)\n",
    "    next_state, reward, done, info = train_envs.step(action.detach().cpu().numpy())\n",
    "\n",
    "    # saving reward and is_terminals\n",
    "    ppo_agent.buffer.insert(state, action, action_logprob, reward, next_state, state_val, done)\n",
    "\n",
    "    state = next_state\n",
    "    time_step += 1\n",
    "    running_reward += reward.sum()\n",
    "    running_episodes += done.sum()\n",
    "\n",
    "    # update PPO agent\n",
    "    if time_step % update_timestep == 0:\n",
    "        ppo_agent.update()\n",
    "\n",
    "    if time_step % summary_freq == 0:\n",
    "        test_ave_reward = evaluate_agent(test_envs, ppo_agent, num_levels=num_test_levels)\n",
    "        print(f\"Timestep: {time_step * num_envs}, \\t\\tAverage Train Reward: {running_reward / running_episodes: .2f}, \\t\\tAverage Test Reward: {test_ave_reward.item(): .2f}\")\n",
    "\n",
    "        running_reward = 0\n",
    "        running_episodes = 0\n",
    "\n",
    "    # save model weights\n",
    "    # if time_step % save_model_freq == 0:\n",
    "    #     print(\"--------------------------------------------------------------------------------------------\")\n",
    "    #     print(\"saving model at : \" + checkpoint_path)\n",
    "    #     ppo_agent.save(checkpoint_path)\n",
    "    #     print(\"model saved\")\n",
    "    #     print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "    #     print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "train_envs.close()"
   ],
   "id": "a366b47f9032ca17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 320000, \t\tAverage Train Reward:  5.55, \t\tAverage Test Reward:  4.85\n",
      "Timestep: 640000, \t\tAverage Train Reward:  6.26, \t\tAverage Test Reward:  5.90\n",
      "Timestep: 960000, \t\tAverage Train Reward:  7.01, \t\tAverage Test Reward:  6.25\n",
      "Timestep: 1280000, \t\tAverage Train Reward:  7.57, \t\tAverage Test Reward:  7.30\n",
      "Timestep: 1600000, \t\tAverage Train Reward:  8.00, \t\tAverage Test Reward:  7.40\n",
      "Timestep: 1920000, \t\tAverage Train Reward:  8.18, \t\tAverage Test Reward:  7.15\n",
      "Timestep: 2240000, \t\tAverage Train Reward:  8.41, \t\tAverage Test Reward:  7.65\n",
      "Timestep: 2560000, \t\tAverage Train Reward:  8.51, \t\tAverage Test Reward:  7.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# update PPO agent\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m time_step \u001B[38;5;241m%\u001B[39m update_timestep \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 27\u001B[0m     \u001B[43mppo_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m time_step \u001B[38;5;241m%\u001B[39m summary_freq \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     30\u001B[0m     test_ave_reward \u001B[38;5;241m=\u001B[39m evaluate_agent(test_envs, ppo_agent, num_levels\u001B[38;5;241m=\u001B[39mnum_test_levels)\n",
      "File \u001B[0;32m~/Computer/RL_research/ppo_procgen_venv/agent/discrete_ppo.py:98\u001B[0m, in \u001B[0;36mPPO.update\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     95\u001B[0m batch_old_states, batch_old_actions, batch_old_logprobs, batch_old_state_values, batch_returns, batch_advantages \u001B[38;5;241m=\u001B[39m sample\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Evaluating old actions and values\u001B[39;00m\n\u001B[0;32m---> 98\u001B[0m logprobs, state_values, dist_entropy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_actions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_old_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_old_actions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m ratios \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mexp(logprobs \u001B[38;5;241m-\u001B[39m batch_old_logprobs\u001B[38;5;241m.\u001B[39mdetach())\n\u001B[1;32m    101\u001B[0m surr1 \u001B[38;5;241m=\u001B[39m ratios \u001B[38;5;241m*\u001B[39m batch_advantages\n",
      "File \u001B[0;32m~/Computer/RL_research/ppo_procgen_venv/agent/models.py:76\u001B[0m, in \u001B[0;36mActorCritic.evaluate_actions\u001B[0;34m(self, inputs, rnn_hxs, masks, action)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mevaluate_actions\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, rnn_hxs, masks, action):\n\u001B[1;32m     74\u001B[0m     state_value, action_dist, rnn_hxs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase(inputs, rnn_hxs, masks)\n\u001B[0;32m---> 76\u001B[0m     dist \u001B[38;5;241m=\u001B[39m \u001B[43mCategorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction_dist\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     action_log_probs \u001B[38;5;241m=\u001B[39m dist\u001B[38;5;241m.\u001B[39mlog_prob(action)\n\u001B[1;32m     78\u001B[0m     dist_entropy \u001B[38;5;241m=\u001B[39m dist\u001B[38;5;241m.\u001B[39mentropy()\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_gen/lib/python3.10/site-packages/torch/distributions/categorical.py:81\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[0;34m(self, probs, logits, validate_args)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     78\u001B[0m batch_shape \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39mndimension() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mSize()\n\u001B[1;32m     80\u001B[0m )\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/rl_gen/lib/python3.10/site-packages/torch/distributions/distribution.py:76\u001B[0m, in \u001B[0;36mDistribution.__init__\u001B[0;34m(self, batch_shape, event_shape, validate_args)\u001B[0m\n\u001B[1;32m     74\u001B[0m         value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, param)\n\u001B[1;32m     75\u001B[0m         valid \u001B[38;5;241m=\u001B[39m constraint\u001B[38;5;241m.\u001B[39mcheck(value)\n\u001B[0;32m---> 76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_is_all_true(valid):\n\u001B[1;32m     77\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     78\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     79\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     82\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     83\u001B[0m             )\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5dccb6624af8814e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
